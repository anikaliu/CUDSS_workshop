---
title: "Explorative data analysis and data visualization in R"
author: "Anika Liu - al862@cam.ac.uk <br> Luca Stefanucci - ls760@cam.ac.uk"
date: "05 Feb 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Index

#### [Short introduction to R](#S1)
#### [Explore the data](#S2)
#### [Visualise dataset](#S3)
#### [Example](#S4)
#### [Practice](#S5)

To fully enjoy the workshop, you should have: 

+ Basic knowledge of R/coding is preferable, but not essential 
+ Fully charged laptop or charger
+ R and Rstudio already installed on your laptop
    + [Install R](https://cran.r-project.org/)
    + [Install Rstudio](https://rstudio.com/products/rstudio/download/#download)

## Short introduction to R {#S1}

R is a free powerful __programming language__ and __environment__. It provides most of the __statistical test__, either built-in or in packages. Moreover, it allows to produce publication-quality __graphs__ (or graphics) accompaining the stats. One of the main reason why R is so widely use in statistics and data analysis is its data structure. It organises elements in __vectors__ and allow to make orperations between them. Vectors are 1-dimension containers for elements that are all of the same type (i.e. logical, integer, double, character, complex or raw). Most likely, in real life problems, data will have a 2-dimesional or matrix-like structure, called __dataframe__ in the R language.

Often paired with R, there is RStudio. Rstudio is a integrated development environment (IDE) developed fo R. When you open RStudio you are going to see 4 panels. The top-left panel is the __text editor__ you can use this to write your script, test it to the console and save it. On the bottom-left, you have the __console__, it executes commands using the __R language as interpreter__, however you can't save the script you run in teh console. Top right, you can see the environment/hisoty tab, when all the variables of this environment are stored and you have a history of the commands you have runned. Ultumately, in the bottom right panel you have the plots, helps.

__Markdown__ is a language to transform a plain text to a rich style text. It is a great way to share your codes. This text is written in markdown, have a look at the raw script.

Now let's start with the course!

## Import the data 

Firstly, let's import the packages we are going to use for this workshop. 
Packages are sets of functions which you can download and they allow you to extend R basic functions. 
In this workshop we will mainly use "tidyverse". Tidyverse is a collection of packages designed to mung and visualise data, key tasks in data science. For more info about Tidyverse go on: https://www.tidyverse.org/

For this workshop we will need the packages `tidyverse` and `reshape2`. Therefore we first install them (this has to be done only once): <br>
N.B. Remove the hash in your code to install them
```{r}
#install.packages('tidyverse')
#install.packages('reshape2')
```
Next, we load the packages into the current environment:
```{r}
library(tidyverse)
library(reshape2)
```

Secondly, we import the data set we want to work on. 
For this workshop, we selected some data publicly available at http://www-huber.embl.de/users/klaus/BasicR/bodyfat.rda. They are made available from Bernd Klaus (EMBL) and used as teaching material for a few R courses. You will find the data in the GitHub repository ("bodyfat.csv"). 

```{r}
bd_fat_df <- read_csv("bodyfat.csv", col_names = TRUE) 
# We import and store the data from the file "bodyfat.csv" in an R dataframe. 
# We set col_names to TRUE in this case, as every column has a header and we want to keep it as column name. 
```
Note that the data frame probably has data collected in U.S., so hight and weight units are in pound and inches.

To understand what a function is doing, how to use it and what variables it takes as input ?<function_name> (e.g. ?read_csv) gives some information about the function and its grammar. help(<function_name>) has a similar use. For example:
```{r}
help(read_csv)
?read_csv
#Or for a broader search:
??read_csv
```

If you can't import the data, most likely the working directory is set to a different path and so R doesn't find the file it is looking for. You have 2 solutions to this problem:
1) change the file path: e.g. `"/path/to/directory/bodyfat.csv"`
2) change the working directory. In this case 2 commands are useful: `getwd()` and `setwd()`. `getwd()` tells you what is the direcotry R is working in (e.g. `~/`) and `setwd()` change the working directory to the one you want (e.g. `"/path/to/directory/"`) so that the file will be in the same directory R is working in. Another alternative is to click your way through: Session-> Set working directory -

## Explore the data {#S2}

It is a good practice to explore a bit your data before starting to work with them. It is helpful to understand what kind of data you are looking at, if there were any mistakes while importing them, if the dataset is "clean" or it needs some wrangling and so on.

Good functions to quickly explore your data frame are `dim()`, `summary()`, `head()`

```{r}
# To show the row and column number of the data frame use dim(). If you are expecting 2000 observations or 30 variables, then you might have spotted a mistake.
dim(bd_fat_df)

# Head is used to visualize the first 10 (in this case) lines of the dataframe
head(bd_fat_df, n = 10) 

# Summary prints a short stats report of the data frame, one per column
summary(bd_fat_df)

```

## Subset the data set

Quite often datasets can grow very big, slowing down all the analysis. So, in these cases it might be handy to take only a part of the dataset. It may be also useful to have a subset of the dataset when you need to check a new script or to create a training dataset (in this last case the subset would be made in a different way).

Let's say we are just interested in people over 40 y.o. and it doesn't make much sense to have the entire dataset in our analysis. How could we subset the dataframe?

```{r}

# To have only those rows that have matching conditions, we can use the filter() function, which is a function of the Tidyverse/dplyr package. 
# In this case, filter asks to return only the rows which have a value over 40 in the column age. We store it in another variable, a new dataframe, called "bd_fat_df40"

bd_fat_df40 <- bd_fat_df %>% 
               filter(age > 40 ) 

# N.B.: In R you can use numbers and some special caracters in the variable name, but not at the beginning of the variable name
```

`%>%`  this symbol is called "pipe". It is a useful way to pass information from one function to the following one. In the previous case I asked to read the bd_fat_df and then to filter out the rows that have a value in the column "age" of 40 y.o. or below.

__Question:__ Have a look at the filter function grammar. How could you have the same result without piping?

``` {r}

# We can also subset according to more than 1 criteria. In this case we ask to return a dataframe of people older than 40 y.o. and taller than 70 inches. To use more than one filter we use Boolean operator AND or OR. (More info here: https://www.datamentor.io/r-programming/operator/)

# AND
bd_fat_df_age_AND_heigth <- bd_fat_df %>% 
                       filter(age > 40 & height > 70.00)
# OR
bd_fat_df_age_OR_heigth <- bd_fat_df %>% 
                       filter(age > 40 | height > 70.00)
```

Sometimes it is convenient to create another observation in the data frame which is derived from one or more observations already in the data set. We can use the mutate function to do this. You can use mutate also for a series of other things (e.g change the class attribute of a vector)

By now, you should be able to check out the `mutate()` funtion documentation. Have a look at it.

``` {r}

bd_fat_df_BMI <- bd_fat_df_age_OR_heigth %>% 
  # mutate here is used to calculate the BMI and add it as a new column to the data frame 
                 mutate(BMI = (weight/height^2) * 703 ) %>% 
  # re-order the data.frame columns as we want. everything() is used to call all the columns which haven't been called before
                 select(age, height, weight, BMI, density, percent.fat, everything()) %>%   
  # this command will sort the data frame columns according to the descending BMI order.
  # arrange is reframing the dataframe and desc() is asking to sort them in an descending order.
                 arrange(desc(BMI))   

```
## Look at the data by groups

One of the strengths of R is the possibility to perform vector operations, such as arithmetics operations or indexes operations. However, sometimes one wants to perform operations just for a sub-group of a vector. This may be particularly useful for categorical variables. A categarical variable, usually, describes a feature of the data and most of the time one wants to preserve these information when performing operations. In this case, group_by (and all its derived functions) can help us in this task.

```{r}
# group_by divides the vector, or the table, in groups defined by one or more variables and performs the operation by these so-defined groups
bd_fat_df_BMI %>% 
      group_by(height <= quantile(height)[3], height > quantile(height)[3]) # here we divide the dataset according to the heights. We will have 2 subgroups having the top 50% heights and the bottom 50% heights

#In this example, group_by creates a new logical-class column which has TRUE where the requirement is satisfied.
```

Often used together with `group_by()` is the `summarise()` fuction, it allows to perform group-wise operations.

```{r}
# We could run summarise on our entire dataset (without group filters).
bd_fat_df_BMI %>% summarise(mean = mean(height), sd = sd(height), mean_BMI = mean(BMI))
# however the results won't be much different from a normal function:
mean(bd_fat_df_BMI$height); sd(bd_fat_df_BMI$height); mean(bd_fat_df_BMI$BMI)
```


```{r}
# More useful is to run summarise after we've grouped our dataset. Here we are grouping according to the hight. Top 50% and bottom 50% to see if there are differences in the BMI
bd_fat_df_BMI %>% 
      group_by(height <= quantile(height)[3], height > quantile(height)[3]) %>% 
      summarise(mean_height = mean(height), sd_height = sd(height), mean_BMI = mean(BMI))

```

## Plot the data set {#S3}

Now that we are happy with data frame structure and content we can start to plot some variables. 

R has a lot of visualization packages, every one has some advantages or may be more helpful to some kind of plot then others. ggplot is a quite versatile package to visualize data, still part of the tidyverse. The strength of ggplot is in its grammar, which is standardized for all kinds of plots and allows the generation of publication-ready figures in a flexible manner.
It is a layer function, which means you can keep on adding data ot it.

Let's start with the basics and create an (empty) graphic with the ggplot function.


```{r plotting}
plot_plain<-ggplot(bd_fat_df_BMI, mapping = aes( y = percent.fat, x=0))

#To look at the plot just call the variable:
plot_plain
```

As you can see the figure currently only contains the axes which are defined through the mapping aesthethics, but there is no actual plot. What if we want to know the distribution of percent.fat? 

One way to show the distribution of a variable is a boxplot which shows the quantiles and the median of the data. The qualtiles are dividing the data in top 25% and bottom 25%, the square has 50% of the entire dataset. Dots are the outliers. (The x axis in this case is not meaningful) 

An alternative to this is a violin plot which shows the underlying distribution.
```{r}

# Boxplot
plot_box<-ggplot(bd_fat_df_BMI, mapping = aes( y = percent.fat, x=0))+
  geom_boxplot() 
plot_box

# Violin plot:
plot_violin<-ggplot(bd_fat_df_BMI, mapping = aes( y = percent.fat, x=0))+
  geom_violin() 
plot_violin

# Or combine both:
plot_combine<-plot_violin+
  geom_boxplot(width=0.6)
plot_combine
```

Let's continue with 2 variables. The basic way to show two continuous variables is a scatter plot with one variable on each axis. We will also add a regression line fitted to the points.
```{r}

plot_scatter <-ggplot(bd_fat_df_BMI, aes(x=BMI, y=percent.fat))+
  #Add the points (scatter plot)
                      geom_point()
plot_scatter

plot_scatter_regression<-plot_scatter+
  #Adding a linear regression in the plot
                      geom_smooth(method= "lm") 
plot_scatter_regression

# Bonus: If you want to find out how the linear regression works, check out this command:
lm(data = bd_fat_df_BMI, BMI ~ percent.fat)
```
__Question:__ We would like to add age in the plot as color of the points. How does this work? Check out the possible aesthetics on the help page of `geom_point()`

Now we would like to generate the same plot, but for all of the measured circumfences in the dataset. One useful way for this is so-called facetting. This requires some reshaping of the data:

```{r melt}
body_fat_melt <- bd_fat_df_BMI %>% 
                 melt(id.vars=c('density', 'percent.fat','age','weight','height')) 

```
Melt is using the columns you are listing to create a unique ID, then it stacks the remaining columns in 2 columns: one containing the variable name and the other containing the relative value (more info: https://i1.wp.com/www.studytrails.com/wp-content/uploads/2016/09/reshape.png).

__Question:__ Compare bd_fat_df_BMI and body_fat_melt (e.g. using head() and dim()). What are the differences?

You'll see that the circumfence columns have been combined to two columns: A variable column containing the original column name and a value column containing the original value. In contrast, the columns defined as ID variables have not been aggregated.

Now we can generate the scatterplot:
```{r facet}
#First we use a similar command as in the previous scatterplot
plot_scatter <- ggplot(body_fat_melt, aes(value, percent.fat))+
                      geom_point() +
                      geom_smooth(method= "lm") 
plot_scatter
#You can see clusters of point, because all the circumfences are plotted. But we want to plot each circumfence in a seperate plot and with an adjusted x axis.

plot_scatter_facet<-plot_scatter+
                      facet_wrap(~variable, scales = 'free_x')
plot_scatter_facet

```

You can see that some of the circumfences are stronger associated with the percent.fat. One way to quantify the strength of association is correlation:
```{r correlation}
# To calculate Pearson correlation of the variable percentage of fat to all the others
cor(bd_fat_df_BMI, bd_fat_df_BMI$percent.fat)
```
__Question:__ Do the derived correlation values agree with your expectations form the plot?

## Excercises - Now it is your turn! {#S5}
For this practical session we have downloaded 2 datasets from the [TidyTuesday commnunity](https://www.tidytuesday.com/).

1. You can find some inspiration from other users on [TidyTuesday Twitter](https://twitter.com/hashtag/tidytuesday?f=live).
2. This [cheatsheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) gives a good overview of commands (we didn't go through all!).
3. Once you are familiar with the basic package, here are some useful [extensions](http://www.ggplot2-exts.org/gallery/)

### Example 1: Passwords and their security
The dataset `password_dataset.csv`has this structure:

|variable          |class     |description |
|:--------------|:--------|:-----------|
|rank              |double    | popularity in their database of released passwords |
|password          |character | Actual text of the password |
|category          |character | What category does the password fall in to?|
|value             |double    | Time to crack by online guessing |
|time_unit         |character | Time unit to match with value |
|offline_crack_sec |double    | Time to crack offline in seconds |
|rank_alt          |double    | Rank 2 |
|strength          |double    | Strength = quality of password where highest value means highest quality |
|font_size         |double    | Used to create the graphic for KIB |

#### __Questions:__ <br>

1. Import the password dataset `password_dataset.csv`
2. Look at the dataset. Does it need to be modified?
2. How many rows are in the dataframe?
3. What is the most secure password?
4. What is the largest password category?
5. Sort the data_base according to the alternative rank
6. What is the category with the highest strength on average?
7. How does the password lenght correlate with its strength?

#### __Tips:__
+ You can use function such as `is.na()` or `na.omit()` to see if there are any missing values
+ You should use `group_by`, `summarise` and `n`
+ You should use `str_length`


### Example 2: Spotify songs and their popularity

|variable                 |class     |description |
|:-----------------|:---------|:-------------------------------------|
|track_id                 |character | Song unique ID|
|track_name               |character | Song Name|
|track_artist             |character | Song Artist|
|track_popularity         |double    | Song Popularity (0-100) where higher is better |
|track_album_id           |character | Album unique ID|
|track_album_name         |character | Song album name |
|track_album_release_date |character | Date when album released |
|playlist_name            |character | Name of playlist |
|playlist_id              |character | Playlist ID|
|playlist_genre           |character | Playlist genre |
|playlist_subgenre        |character | Playlist subgenre|
|danceability             |double    | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. |
|energy                   |double    | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. |
|key                      |double    | The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1. |
|loudness                 |double    | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.|
|mode                     |double    | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.|
|speechiness              |double    | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
|acousticness             |double    | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.|
|instrumentalness         |double    | Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. |
|liveness                 |double    | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. |
|valence                  |double    | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). |
|tempo                    |double    | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. |
|duration_ms              |double    | Duration of song in milliseconds |

#### __Questions:__ <br>

Import `spotify_songs.csv`

1. Which artist has the most popular songs on average in the 2000s?
2. Which artist made songs in most of the years?
3. Who are the top 5 track artist by playlist genre in the last 10 years?
4. How are songs from Kendrick Lamar different to those from Rihanna?
5. Are the song features (speechiness, acousticness,...) correlated?
6. How did danceability in EDM evolve over time?


#### Tips:__ <br>
You might have to change the variable classes to numeric. One way to do it is shown in the example, but you can also use mutate() for example.
```{r Example}
spotify_songs<-read_csv('spotify_songs.csv')
#I wanty to look only at the songs from 2019 and want to have the most speechy songs on top
spotify_2019<-spotify_songs%>%filter(track_album_release_date=='2019')%>%arrange(speechiness)
#Now the plotting:
ggplot(spotify_2019,aes(as.numeric(loudness), as.numeric(duration_ms)))+
  geom_point(aes(color=as.numeric(speechiness)), alpha=0.8)+
  facet_wrap(~playlist_genre)


ggsave('~/spotify_songs_example.pdf')
```



